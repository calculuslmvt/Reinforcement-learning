{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openaigym_ayush.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV-_5VJsPWeT",
        "outputId": "b52e2a89-2de7-4b32-d540-56bf00a9f98f"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4g6SYTbPdWs"
      },
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "import random \n",
        "import time \n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9XmkYIWQQDO"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvol3zyQYg6",
        "outputId": "15c204b7-a8f2-48a6-bef4-c46a4004d5ac"
      },
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        " \n",
        "q_table = np.zeros((state_space_size , action_space_size))\n",
        "print(q_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SwEwrUQ-GG"
      },
      "source": [
        "num_episode = 10000\n",
        "max_steps_per_episode = 100 # so if agent has not reached frisby by end of 100 episodes then episode will end  with 0 rewards\n",
        " \n",
        "learning_rate = 0.1  # alpha \n",
        "discount_rate = 0.99 # gamma\n",
        " \n",
        " \n",
        "exploration_rate =1       # epsilon greedy strategy is being used here \n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ted-S0S7vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728e5aef-3d56-4c33-97e4-5ef34147e85a"
      },
      "source": [
        "rewards_all_episodes = [] # 10000 rewards\n",
        "#q - learning algorithm \n",
        " \n",
        "for episode in range(num_episode):  #loop for each episode(so 10000 it should run as defined in uper block)\n",
        "    state = env.reset()   # environment is reset after every episode (detailed reason is yet to be known ie how knownledge on one episode is transfered to another)\n",
        "    done = False   #this just checks if reset was done or not \n",
        "    reward_current_episode = 0 \n",
        " \n",
        "    for step in range(max_steps_per_episode): # this loop tells about what happens at every time step \n",
        "        #exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0,1)\n",
        "        if exploration_rate_threshold > exploration_rate : \n",
        "            action = np.argmax(q_table[state , : ])  # so exploitation occurs and action is taken according to max q value in the table \n",
        "        else :\n",
        "           action = env.action_space.sample()   # here exploration occurs and random action taken \n",
        " \n",
        "           new_state , reward , done , info = env.step(action) # particular action is send to the env object \n",
        " \n",
        "           #update q - table for q(s,a)\n",
        " \n",
        "           q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action]) #now here max means that\n",
        "           #1)when you have got reward for next state then now we to update q table 2) so max here means that step function gives the next state depending on action choosed\n",
        "           #so in the next state there would be q values depending on differents action so we have to max q - value from current table to update q-table for current state \n",
        "           #that means to update q value of current state ,action we need q value of next state,action in currently non - updated q-table \n",
        " \n",
        "           state  = new_state \n",
        "           reward_current_episode+= reward \n",
        " \n",
        "           if done == True:\n",
        "              break ; \n",
        " \n",
        "      #exploration rate decay \n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
        "        \n",
        " \n",
        "    rewards_all_episodes.append(reward_current_episode)\n",
        " \n",
        "            # Calculate and print the average reward per thousand episodes\n",
        "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),1)\n",
        "    count = 1000\n",
        " \n",
        "    print(\"********Average reward per thousand episodes********\\n\")\n",
        "    for r in rewards_per_thousand_episodes:\n",
        "          print(count, \": \", str(sum(r/100)))\n",
        "          count += 1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n",
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.3200000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlTyQd7-eiM6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U01YAQxGM6I9",
        "outputId": "900803dc-cb26-42fb-b88b-54bba870db2d"
      },
      "source": [
        "# Print updated Q-table\n",
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********Q-table********\n",
            "\n",
            "[[0.22392889 0.21229789 0.21858401 0.21754651]\n",
            " [0.12621272 0.10368005 0.13545152 0.20535507]\n",
            " [0.17168289 0.15011645 0.14936508 0.15362977]\n",
            " [0.0536772  0.06225852 0.06304053 0.1139011 ]\n",
            " [0.22949612 0.22225902 0.18840044 0.15176949]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.16696032 0.03494931 0.15717628 0.04692042]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.17477536 0.15259371 0.13473683 0.25019064]\n",
            " [0.21864501 0.27958727 0.24878489 0.16851781]\n",
            " [0.41626782 0.37357903 0.33232249 0.1031536 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.19652752 0.25417947 0.35632353 0.19606551]\n",
            " [0.32354047 0.48216144 0.66516063 0.62855856]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4fiy_GbNSCS",
        "outputId": "a8e7aee5-9bd6-4975-9285-f721255ad3a0"
      },
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtaDR2RmNeZE"
      },
      "source": [
        "total_episodes = 15000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA2JruD5NpCb",
        "outputId": "ad924d98-785f-4d0a-810e-64d18a5b16fe"
      },
      "source": [
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)   \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.4881333333333333\n",
            "[[8.20032681e-02 6.95430875e-02 6.61235207e-02 6.70383332e-02]\n",
            " [1.01036245e-02 1.11522224e-02 1.71770760e-02 3.65486358e-02]\n",
            " [1.99317888e-02 1.32322958e-02 1.42054428e-02 1.44918704e-02]\n",
            " [9.69680782e-03 1.74167238e-03 4.90715860e-03 1.42272179e-02]\n",
            " [7.52715637e-02 6.81018381e-02 1.81167384e-02 7.95319762e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.19221752e-04 2.51909170e-09 4.14102148e-02 4.92825740e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.60503260e-02 1.85820162e-02 1.84498198e-02 1.51066098e-01]\n",
            " [6.64204796e-02 5.56036173e-01 6.20921576e-02 2.36645164e-02]\n",
            " [2.23782667e-02 9.85198564e-03 3.82597717e-01 2.45739104e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.32170321e-02 2.97628976e-02 8.74393397e-01 1.49637797e-04]\n",
            " [1.74088445e-01 9.94072815e-01 2.36827641e-01 2.54279396e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoF1CQJwN5HR",
        "outputId": "6acf46ac-b25b-462c-9f9a-aa718f48c808"
      },
      "source": [
        "# Print updated Q-table\n",
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(qtable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********Q-table********\n",
            "\n",
            "[[8.20032681e-02 6.95430875e-02 6.61235207e-02 6.70383332e-02]\n",
            " [1.01036245e-02 1.11522224e-02 1.71770760e-02 3.65486358e-02]\n",
            " [1.99317888e-02 1.32322958e-02 1.42054428e-02 1.44918704e-02]\n",
            " [9.69680782e-03 1.74167238e-03 4.90715860e-03 1.42272179e-02]\n",
            " [7.52715637e-02 6.81018381e-02 1.81167384e-02 7.95319762e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.19221752e-04 2.51909170e-09 4.14102148e-02 4.92825740e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.60503260e-02 1.85820162e-02 1.84498198e-02 1.51066098e-01]\n",
            " [6.64204796e-02 5.56036173e-01 6.20921576e-02 2.36645164e-02]\n",
            " [2.23782667e-02 9.85198564e-03 3.82597717e-01 2.45739104e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.32170321e-02 2.97628976e-02 8.74393397e-01 1.49637797e-04]\n",
            " [1.74088445e-01 9.94072815e-01 2.36827641e-01 2.54279396e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW8Jr9iIODjW",
        "outputId": "9a94ccf6-0232-4aea-e05a-f7b8e432c26e"
      },
      "source": [
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 3\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 3\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 13\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 6\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}